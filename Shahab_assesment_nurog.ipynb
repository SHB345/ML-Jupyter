{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Shahab_assesment_nurog.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfPD+eQ9EDD6Q5/TxWwWYa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Code Explanation**\n","\n","The environment used in this assignment is **CartPole-v0**. In this environment, a pole is attached to a cart and the cart moves along a friction less track. The goal is to balance the pole on the cart without falling with certain caveats.\n","\n","The agent used in this assignment is DQNAgent, which interact with the cartpole environment according the assigned policy and learns if the action results in a reward.\n","\n","The SequentialMemory stores 50000 steps or actions that the agent take according to the BoltzmannQPolicy and stores all the observations whether resulting in a reward or not, i.e the agent moves along the track towards left and right according to the policy and results of reward or no reward are stored.\n","\n","We train the agent so that the agent learns for itself which action to take, i.e move left or right. The more the agent is trained, the better the performance will be.\n","\n","After training, we test the agent for some episodes to see how well it performs."],"metadata":{"id":"kF9iK_J_MMqw"}},{"cell_type":"code","source":["# First we install the required modules\n","\n","!pip install keras-rl2\n","!pip install gym"],"metadata":{"id":"EDt7WHAZOdX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Then import the necessery tools\n","\n","import gym\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","from rl.agents.dqn import DQNAgent\n","from rl.policy import BoltzmannQPolicy\n","from rl.memory import SequentialMemory\n"],"metadata":{"id":"8VLAPq0dnsjl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the environment and extract the number of actions.\n","\n","ENV_NAME = 'CartPole-v0'\n","env = gym.make(ENV_NAME)\n","np.random.seed(123)\n","env.seed(123)\n","nb_actions = env.action_space.n"],"metadata":{"id":"JPwQM1JPOqWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Next, we build a very simple model.\n","model = Sequential()\n","model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","model.add(Dense(16))\n","model.add(Activation('relu'))\n","model.add(Dense(16))\n","model.add(Activation('relu'))\n","model.add(Dense(16))\n","model.add(Activation('relu'))\n","model.add(Dense(nb_actions))\n","model.add(Activation('linear'))\n","print(model.summary())\n","\n","# Now, we configure and compile our agent.\n","\n","memory = SequentialMemory(limit=50000, window_length=1) # limit shows how many entries can be held, while window_length shows how many entries make a state\n","policy = BoltzmannQPolicy()\n","dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=30,\n","               target_model_update=1e-2, policy=policy) \n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","\n","# Train our agent\n","\n","dqn.fit(env, nb_steps=2000, visualize=False, verbose=2) # you can play with the nb_steps and see how well the model performs for larger values"],"metadata":{"id":"k0VWwww3n01M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finally, test the agent\n","\n","dqn.test(env, nb_episodes=5, visualize=False)"],"metadata":{"id":"H3qtz_BRKXup"},"execution_count":null,"outputs":[]}]}